{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeAm9EuGWA8xjBfozz9OB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocky12312/Sentiment-analysis/blob/master/Sentiment_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHLkDu1mVohK",
        "colab_type": "code",
        "outputId": "aecad54c-51cb-4257-a95c-4573f6634e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "golT3kIgZcMF",
        "colab_type": "code",
        "outputId": "e925a5ab-0d37-4c4a-c254-c4e46fba016a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/Tweet_sentiment_extraction/tse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Tweet_sentiment_extraction/tse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6CEPB5eZuAw",
        "colab_type": "code",
        "outputId": "25c8bfb0-55b8-4d09-9096-93952c9acc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nihLCsPY4jan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBjgW6225P04",
        "colab_type": "text"
      },
      "source": [
        "Reading the input dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3VXmcYrZ5ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "df_sample_submission = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V7mDrtA5VVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8a73b1f0-536c-4af4-d4d8-13f59ccf0e98"
      },
      "source": [
        "df_train.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_2cjJ05O6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fef75623-74d4-48cd-a614-650681c09737"
      },
      "source": [
        "df_test.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>Shanghai is also really exciting (precisely -...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                               text sentiment\n",
              "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
              "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
              "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
              "3  01082688c6                                        happy bday!  positive\n",
              "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS33Hw6E5gHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "39dc43d2-8d8e-4788-bab7-fcaa913476ee"
      },
      "source": [
        "df_sample_submission.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  selected_text\n",
              "0  f87dea47db            NaN\n",
              "1  96d74cb729            NaN\n",
              "2  eee518ae67            NaN\n",
              "3  01082688c6            NaN\n",
              "4  33987a8ee5            NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ZulENF5lQW",
        "colab_type": "text"
      },
      "source": [
        "Basically in this problem what we have to do is to train a model on our training data such that we can find the selected_text data column corresponding to our test data which is basically the part of sentiment text, which help in determination whether our sentiment is neutral,positive or negative.\n",
        "\n",
        "This is like question answering task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ZLJ9qx6anv",
        "colab_type": "text"
      },
      "source": [
        "Getting the info about the dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei_0VUI75kVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "6d765705-aeb7-4420-b16b-da2b59b7129f"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   textID         27481 non-null  object\n",
            " 1   text           27480 non-null  object\n",
            " 2   selected_text  27480 non-null  object\n",
            " 3   sentiment      27481 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 858.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KerXqksH6iQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "ba3755cc-1df9-4401-ba35-55ee58d3807d"
      },
      "source": [
        "df_test.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3534 entries, 0 to 3533\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   textID     3534 non-null   object\n",
            " 1   text       3534 non-null   object\n",
            " 2   sentiment  3534 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 83.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6A3lSE-6lPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "93b793dd-f868-4eb3-cd7b-cb14b3786c5d"
      },
      "source": [
        "df_train.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "textID           0\n",
              "text             1\n",
              "selected_text    1\n",
              "sentiment        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vioXNNV7CqO",
        "colab_type": "text"
      },
      "source": [
        "As we know that we have the text and selected_text column are of type string so better is to convert them to string type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlP_IXa61Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[\"text\"] = df_train[\"text\"].astype(str)\n",
        "df_train[\"selected_text\"] = df_train[\"selected_text\"].astype(str)\n",
        "df_train[\"sentiment\"] = df_train[\"sentiment\"].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU8LtSliDuQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test[\"text\"] = df_test[\"text\"].astype(str)\n",
        "df_test[\"sentiment\"] = df_test[\"sentiment\"].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eDbR7kz7v1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "ee72315e-0837-4deb-af2a-cf735bf368dc"
      },
      "source": [
        "df_train.isnull().sum()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "textID           0\n",
              "text             0\n",
              "selected_text    0\n",
              "sentiment        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nzxIkWO7153",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Cleaning the text\n",
        "def Cleaning_text(text_data):\n",
        "  for i in range(len(text_data)):\n",
        "    #Lowering the text and removing all the hypertexts\n",
        "    text = text_data[i].lower()\n",
        "    text = re.sub(r\"https?://www\\.\\S+\\.com\",\"\",text)\n",
        "    #Splitting the text into tokens\n",
        "    tokens = text.split()\n",
        "    #Preparing regex for character filtering\n",
        "    re_punct = re.compile(\"[%s]\"% re.escape(string.punctuation))\n",
        "    tokens = [re_punct.sub(\" \", word) for word in tokens]\n",
        "    #Now joining tokens back to form sentence\n",
        "    text_data[i] = \" \".join(tokens)\n",
        "  return text_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4yhcFHW8BUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[\"text\"] = Cleaning_text(df_train[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0xMD5gf8EQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[\"selected_text\"] = Cleaning_text(df_train[\"selected_text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912TGsib8Yem",
        "colab_type": "text"
      },
      "source": [
        "Now as our data is tweet data so it is very much possible that data will be containing links, Shortforms, Contractions along with usual textual noise so one of our main step will be text cleaning.\n",
        "\n",
        "Mainly we will be cleaning the text and selected text column as for our sentiment column we only need to encode it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsnSEOFZ8TjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "5446f59b-ac85-428c-cca7-656b80fd4449"
      },
      "source": [
        "df_train[\"sentiment\"].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral     11118\n",
              "positive     8582\n",
              "negative     7781\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5uOL-mG8p79",
        "colab_type": "text"
      },
      "source": [
        "Plotting the classwise counts of sentiments from training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAMxrUm58iqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "1764e235-b5b4-425a-a5a3-53d520363c9f"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style = \"darkgrid\")\n",
        "ax = sns.countplot(x = \"sentiment\",data = df_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZqyOxx985Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "32304b0c-89d6-475c-9716-2ff56c6891d1"
      },
      "source": [
        "ax1 = sns.countplot(x = \"sentiment\",data = df_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olvTbT9Y9C-A",
        "colab_type": "text"
      },
      "source": [
        "From the above plots it is clear that we have imbalanced data both in our training and testing data but as our task is not to predict the labels but to extract the selected_text from text which help in providing sentiment so we will not go for balancing the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXGvux_eBht-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2504589c-30b9-402a-ebc2-04891ffc3f06"
      },
      "source": [
        "df_train.head(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>i d have responded  if i were going</td>\n",
              "      <td>i d have responded  if i were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>sooo sad i will miss you here in san diego</td>\n",
              "      <td>sooo sad</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview  leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>sons of       why couldn t they put them on th...</td>\n",
              "      <td>sons of</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiRe3YtR9RKE",
        "colab_type": "text"
      },
      "source": [
        "So as we have preprocessed our data so now what we will do is manipulate our data in such a way that so that we can feed it to our model(roBERTa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSXEtkx6aJYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *\n",
        "import tokenizers\n",
        "\n",
        "#Model max length\n",
        "MAX_LEN = 140\n",
        "PATH = \"../tse/tf-roberta/\"\n",
        "#Creating the tokenizer\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+\"vocab-roberta-base.json\", \n",
        "    merges_file=PATH+\"merges-roberta-base.txt\", \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {\"positive\": 1313,\"negative\": 2430, \"neutral\": 7974}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_9ul9_-lYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08008d7e-3a0b-4dbd-cfb9-ca66ff3dbcfa"
      },
      "source": [
        "df_train.shape[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G57mt42HAHUZ",
        "colab_type": "text"
      },
      "source": [
        "We will now convert the training data into arrays that roBERTa can understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irCb9HIaaPbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = np.ones((df_train.shape[0],MAX_LEN),dtype=\"int32\")\n",
        "attention_mask = np.zeros((df_train.shape[0],MAX_LEN),dtype=\"int32\")\n",
        "token_type_ids = np.zeros((df_train.shape[0],MAX_LEN),dtype=\"int32\")\n",
        "start_tokens = np.zeros((df_train.shape[0],MAX_LEN),dtype=\"int32\")\n",
        "end_tokens = np.zeros((df_train.shape[0],MAX_LEN),dtype=\"int32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz7xlv4hbXXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in range(df_train.shape[0]):\n",
        "    \n",
        "    #Finding the text1 and text2 overlap(text and selected_text)\n",
        "    text1 = \" \"+\" \".join(df_train.loc[k,\"text\"].split())\n",
        "    text2 = \" \".join(df_train.loc[k,\"selected_text\"].split())\n",
        "    #Finding text2 starting id in text1(selected_text in text)\n",
        "    idx = text1.find(text2)\n",
        "    #Creating a chars array and imputing it with ones and zeros corresponding to idx\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==\" \": chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    #ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    #Start end tokens\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[df_train.loc[k,\"sentiment\"]]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Fs9M7nb1Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids_t = np.ones((df_test.shape[0],MAX_LEN),dtype = \"int32\")\n",
        "attention_mask_t = np.zeros((df_test.shape[0],MAX_LEN),dtype = \"int32\")\n",
        "token_type_ids_t = np.zeros((df_test.shape[0],MAX_LEN),dtype = \"int32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5hQch0_Ex8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in range(df_test.shape[0]):\n",
        "        \n",
        "    #input id's\n",
        "    text1 = \" \"+\" \".join(df_test.loc[k,\"text\"].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[df_test.loc[k,\"sentiment\"]]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7tvw9FIGQf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H03KrtvcFPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH + \"config-roberta-base.json\")\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH + \"pretrained-roberta-base.h5\",config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids = tok)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(256, 2,padding = \"same\")(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding = \"same\")(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation(\"softmax\")(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(256, 2, padding = \"same\")(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding = \"same\")(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation(\"softmax\")(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUgiXj1cRax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_FWEtxYcU3J",
        "colab_type": "code",
        "outputId": "5463fd9c-98ab-4369-e400-5ee26393715a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "jac = []; VER=\"v0\"; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=43)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,df_train.sentiment.values)):\n",
        "\n",
        "    print(\"#@\"*25)\n",
        "    print(\"### FOLD %i\"%(fold+1))\n",
        "    print(\"#@\"*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"%s-roberta-%i.h5\"%(VER,fold), monitor = \"val_loss\", verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode = \"auto\", save_freq = \"epoch\")\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    print(\"Loading the model\")\n",
        "    model.load_weights(\"%s-roberta-%i.h5\"%(VER,fold))\n",
        "    \n",
        "    print(\"Predicting OOF\")\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print(\"Predicting Test0\")\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    #Displaying fold jaccard\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = df_train.loc[k,\"text\"]\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(df_train.loc[k,\"text\"].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,df_train.loc[k,\"selected_text\"]))\n",
        "    jac.append(np.mean(all))\n",
        "    print(\"FOLD %i Jaccard =\"%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "687/687 [==============================] - ETA: 0s - loss: 2.1140 - activation_loss: 1.0757 - activation_1_loss: 1.0383\n",
            "Epoch 00001: val_loss improved from inf to 1.69247, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 384s 558ms/step - loss: 2.1140 - activation_loss: 1.0757 - activation_1_loss: 1.0383 - val_loss: 1.6925 - val_activation_loss: 0.8529 - val_activation_1_loss: 0.8396\n",
            "Epoch 2/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.6346 - activation_loss: 0.8397 - activation_1_loss: 0.7950\n",
            "Epoch 00002: val_loss improved from 1.69247 to 1.63287, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 383s 558ms/step - loss: 1.6346 - activation_loss: 0.8397 - activation_1_loss: 0.7950 - val_loss: 1.6329 - val_activation_loss: 0.8277 - val_activation_1_loss: 0.8051\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.4962 - activation_loss: 0.7708 - activation_1_loss: 0.7254\n",
            "Epoch 00003: val_loss did not improve from 1.63287\n",
            "687/687 [==============================] - 379s 552ms/step - loss: 1.4962 - activation_loss: 0.7708 - activation_1_loss: 0.7254 - val_loss: 1.6654 - val_activation_loss: 0.8555 - val_activation_1_loss: 0.8099\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 182ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7076149409261128\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1011 - activation_loss: 1.0726 - activation_1_loss: 1.0285\n",
            "Epoch 00001: val_loss improved from inf to 1.72092, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 395s 575ms/step - loss: 2.1011 - activation_loss: 1.0726 - activation_1_loss: 1.0285 - val_loss: 1.7209 - val_activation_loss: 0.8793 - val_activation_1_loss: 0.8416\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6361 - activation_loss: 0.8410 - activation_1_loss: 0.7952\n",
            "Epoch 00002: val_loss improved from 1.72092 to 1.62311, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 396s 575ms/step - loss: 1.6361 - activation_loss: 0.8410 - activation_1_loss: 0.7952 - val_loss: 1.6231 - val_activation_loss: 0.8511 - val_activation_1_loss: 0.7720\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.4799 - activation_loss: 0.7632 - activation_1_loss: 0.7167\n",
            "Epoch 00003: val_loss did not improve from 1.62311\n",
            "688/688 [==============================] - 391s 569ms/step - loss: 1.4799 - activation_loss: 0.7632 - activation_1_loss: 0.7167 - val_loss: 1.6644 - val_activation_loss: 0.8745 - val_activation_1_loss: 0.7899\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 182ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 181ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.7037940770681066\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1443 - activation_loss: 1.0942 - activation_1_loss: 1.0501\n",
            "Epoch 00001: val_loss improved from inf to 1.65747, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 396s 575ms/step - loss: 2.1443 - activation_loss: 1.0942 - activation_1_loss: 1.0501 - val_loss: 1.6575 - val_activation_loss: 0.8445 - val_activation_1_loss: 0.8130\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6724 - activation_loss: 0.8585 - activation_1_loss: 0.8139\n",
            "Epoch 00002: val_loss improved from 1.65747 to 1.64683, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 396s 576ms/step - loss: 1.6724 - activation_loss: 0.8585 - activation_1_loss: 0.8139 - val_loss: 1.6468 - val_activation_loss: 0.8549 - val_activation_1_loss: 0.7919\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5266 - activation_loss: 0.7878 - activation_1_loss: 0.7387\n",
            "Epoch 00003: val_loss improved from 1.64683 to 1.59133, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 395s 575ms/step - loss: 1.5266 - activation_loss: 0.7878 - activation_1_loss: 0.7387 - val_loss: 1.5913 - val_activation_loss: 0.8236 - val_activation_1_loss: 0.7677\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 181ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.7087598839267137\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1101 - activation_loss: 1.0804 - activation_1_loss: 1.0297\n",
            "Epoch 00001: val_loss improved from inf to 1.72188, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 395s 574ms/step - loss: 2.1101 - activation_loss: 1.0804 - activation_1_loss: 1.0297 - val_loss: 1.7219 - val_activation_loss: 0.8609 - val_activation_1_loss: 0.8610\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6342 - activation_loss: 0.8505 - activation_1_loss: 0.7837\n",
            "Epoch 00002: val_loss improved from 1.72188 to 1.65679, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 395s 574ms/step - loss: 1.6342 - activation_loss: 0.8505 - activation_1_loss: 0.7837 - val_loss: 1.6568 - val_activation_loss: 0.8284 - val_activation_1_loss: 0.8284\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.4904 - activation_loss: 0.7776 - activation_1_loss: 0.7128\n",
            "Epoch 00003: val_loss improved from 1.65679 to 1.63893, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 395s 574ms/step - loss: 1.4904 - activation_loss: 0.7776 - activation_1_loss: 0.7128 - val_loss: 1.6389 - val_activation_loss: 0.8213 - val_activation_1_loss: 0.8177\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 184ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 181ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.7096819591619274\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1322 - activation_loss: 1.0919 - activation_1_loss: 1.0403\n",
            "Epoch 00001: val_loss improved from inf to 1.71945, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 398s 578ms/step - loss: 2.1322 - activation_loss: 1.0919 - activation_1_loss: 1.0403 - val_loss: 1.7194 - val_activation_loss: 0.8998 - val_activation_1_loss: 0.8197\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6401 - activation_loss: 0.8421 - activation_1_loss: 0.7980\n",
            "Epoch 00002: val_loss improved from 1.71945 to 1.69523, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 398s 579ms/step - loss: 1.6401 - activation_loss: 0.8421 - activation_1_loss: 0.7980 - val_loss: 1.6952 - val_activation_loss: 0.8812 - val_activation_1_loss: 0.8140\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5095 - activation_loss: 0.7757 - activation_1_loss: 0.7339\n",
            "Epoch 00003: val_loss improved from 1.69523 to 1.68723, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 399s 580ms/step - loss: 1.5095 - activation_loss: 0.7757 - activation_1_loss: 0.7339 - val_loss: 1.6872 - val_activation_loss: 0.8644 - val_activation_1_loss: 0.8228\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 182ms/step\n",
            ">>>> FOLD 5 Jaccard = 0.6981486268573044\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCNv-xibcvgC",
        "colab_type": "code",
        "outputId": "8e7a471a-61ef-43a5-aefe-b9f02bf084dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"overall 5fold cv Jaccard =\",np.mean(jac))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall 5fold cv Jaccard = 0.705599897588033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2dVIz6cwr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = df_test.loc[k,\"text\"]\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(df_test.loc[k,\"text\"].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NTy_dyec7hW",
        "colab_type": "code",
        "outputId": "050cbcad-6ea4-47d7-c984-d7d26266d426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "source": [
        "df_test[\"selected_text\"] = all\n",
        "df_test[[\"textID\",\"selected_text\"]].to_csv(\"submission.csv\",index=False)\n",
        "pd.set_option(\"max_colwidth\", 60)\n",
        "df_test.sample(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>303a1773cf</td>\n",
              "      <td>every day of my life lately.    i feel ya girl &lt;3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>every day of my life lately. i feel ya girl &lt;3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1942</th>\n",
              "      <td>79c57e46eb</td>\n",
              "      <td>Hey, sorry I got off last night!</td>\n",
              "      <td>negative</td>\n",
              "      <td>sorry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>e2505e6219</td>\n",
              "      <td>#SanctuarySunday  thanks for joining on #SanctuarySunday...</td>\n",
              "      <td>positive</td>\n",
              "      <td>thanks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484</th>\n",
              "      <td>8ba9d3448f</td>\n",
              "      <td>#warmfuzzies to you, my friend</td>\n",
              "      <td>positive</td>\n",
              "      <td>#warmfuzzies to you, my friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>646d1df036</td>\n",
              "      <td>@_Freya Good Night</td>\n",
              "      <td>positive</td>\n",
              "      <td>good night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>ddbc8dba62</td>\n",
              "      <td>@ lovelytrinkets I like the way you worded that about Ro...</td>\n",
              "      <td>positive</td>\n",
              "      <td>like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2655</th>\n",
              "      <td>9f69f15ca6</td>\n",
              "      <td>what`s the matter?</td>\n",
              "      <td>neutral</td>\n",
              "      <td>what`s the matter?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>a4388148b8</td>\n",
              "      <td>hahaha n I`m a do the same for u</td>\n",
              "      <td>neutral</td>\n",
              "      <td>hahaha n i`m a do the same for u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2814</th>\n",
              "      <td>d6606e8167</td>\n",
              "      <td>Bout to knock out. Feelin a lil sick  peace y`all</td>\n",
              "      <td>negative</td>\n",
              "      <td>sick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>8255b23e72</td>\n",
              "      <td>I`m not ready for my baby to be 3 tomorrow  she is growi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>she is growing so fast....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>aff740a028</td>\n",
              "      <td>09/09 +full moon+ my lovely friends and family= my b-day...</td>\n",
              "      <td>positive</td>\n",
              "      <td>lovely friends and family= my b-day wish came true;)&lt;3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>aa17c6f797</td>\n",
              "      <td>_buckley Good morning from sunny London...sounds like an...</td>\n",
              "      <td>positive</td>\n",
              "      <td>good morning from sunny london...sounds like an oxymoro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1412</th>\n",
              "      <td>92866f7b8c</td>\n",
              "      <td>Thank you!</td>\n",
              "      <td>positive</td>\n",
              "      <td>thank you!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>cf3b068b03</td>\n",
              "      <td>Is watching acoustic performances! &amp; In the mood for a g...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>is watching acoustic performances! &amp; in the mood for a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>d546f8c422</td>\n",
              "      <td>Amazing day with my boyfriend. He`s a good drummer.</td>\n",
              "      <td>positive</td>\n",
              "      <td>amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2525</th>\n",
              "      <td>94f554867c</td>\n",
              "      <td>Just got a call from my Realtor saying I have another sh...</td>\n",
              "      <td>positive</td>\n",
              "      <td>interest.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2078</th>\n",
              "      <td>20a5175d49</td>\n",
              "      <td>I will have to see the unveiling of your new LR arrange...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i will have to see the unveiling of your new lr arrange...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>88c7658eb5</td>\n",
              "      <td>I ate too many kisses.</td>\n",
              "      <td>negative</td>\n",
              "      <td>i ate too many kisses.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867</th>\n",
              "      <td>e5a26fb323</td>\n",
              "      <td>lol! woow okay its not that big of a deal</td>\n",
              "      <td>neutral</td>\n",
              "      <td>lol! woow okay its not that big of a deal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>85eb03d6b8</td>\n",
              "      <td>Cant Thank YOu Enough, WHo is NITIN BETWEEN?</td>\n",
              "      <td>positive</td>\n",
              "      <td>cant thank you enough,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>c50bdd4567</td>\n",
              "      <td>thats so cool</td>\n",
              "      <td>positive</td>\n",
              "      <td>thats so cool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2171</th>\n",
              "      <td>30e7967de4</td>\n",
              "      <td>To every mom, mommy and mother, Happy Mother`s Day   Hop...</td>\n",
              "      <td>positive</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2873</th>\n",
              "      <td>34533b5903</td>\n",
              "      <td>FINALLY-- Now I am me again.  I had to use internet expl...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>finally-- now i am me again. i had to use internet expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2051</th>\n",
              "      <td>c625cee17b</td>\n",
              "      <td>you might also want to include 'never wear a moonwolf' ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>you might also want to include 'never wear a moonwolf' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>910</th>\n",
              "      <td>ff9722bca7</td>\n",
              "      <td>Blah to car repairs... almost 600 dollars...   Shopping ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>blah</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                                selected_text\n",
              "580   303a1773cf  ...               every day of my life lately. i feel ya girl <3\n",
              "1942  79c57e46eb  ...                                                        sorry\n",
              "289   e2505e6219  ...                                                       thanks\n",
              "1484  8ba9d3448f  ...                               #warmfuzzies to you, my friend\n",
              "1899  646d1df036  ...                                                   good night\n",
              "359   ddbc8dba62  ...                                                         like\n",
              "2655  9f69f15ca6  ...                                           what`s the matter?\n",
              "558   a4388148b8  ...                             hahaha n i`m a do the same for u\n",
              "2814  d6606e8167  ...                                                         sick\n",
              "1389  8255b23e72  ...                                   she is growing so fast....\n",
              "268   aff740a028  ...   lovely friends and family= my b-day wish came true;)<3 ...\n",
              "859   aa17c6f797  ...   good morning from sunny london...sounds like an oxymoro...\n",
              "1412  92866f7b8c  ...                                                   thank you!\n",
              "50    cf3b068b03  ...   is watching acoustic performances! & in the mood for a ...\n",
              "1532  d546f8c422  ...                                                      amazing\n",
              "2525  94f554867c  ...                                                    interest.\n",
              "2078  20a5175d49  ...   i will have to see the unveiling of your new lr arrange...\n",
              "1161  88c7658eb5  ...                                       i ate too many kisses.\n",
              "867   e5a26fb323  ...                    lol! woow okay its not that big of a deal\n",
              "1344  85eb03d6b8  ...                                       cant thank you enough,\n",
              "25    c50bdd4567  ...                                                thats so cool\n",
              "2171  30e7967de4  ...                                                        happy\n",
              "2873  34533b5903  ...   finally-- now i am me again. i had to use internet expl...\n",
              "2051  c625cee17b  ...   you might also want to include 'never wear a moonwolf' ...\n",
              "910   ff9722bca7  ...                                                         blah\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OY4OcJ8YJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}