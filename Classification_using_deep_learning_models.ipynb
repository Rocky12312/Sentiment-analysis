{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the movie reviews data\n",
    "review_data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "review_data.head(5)\n",
    "#The dataset contain two columns one for the review text and other for the sentiments(whether people like it or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#Getting the info of data using info method\n",
    "review_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review data contain 50000 entries for both reviews and sentiments\n",
    "Both reviews and sentiment are non null objects(datatype=object)\n",
    "\n",
    "This is a dataset for binary sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now checking for the null value in dataset\n",
    "review_data.isnull().sum()\n",
    "#So as we have checked that our dataset contains zero null value for the review and sentiment columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data[\"review\"][1]\n",
    "#As we can see that our data contains punctuations, html tags, stopwords(not of much importance) so first we need to preprocess our text before feeding it to model(ml or dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see that we have fully balanced dataset(The number of positive examples are equal to the number of negative examples) so we need not to perform any of operating like upsampling,downsampling or use smote algorithm(Borderline smote) for sampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3f87534d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVO0lEQVR4nO3df7CeZX3n8ffHANafBSSwSKBx2XQrao2SAZTdHZUdDMy0oAULUyRQZmJdcGp/7Ba7O4VKabWiTnWVFmtK2FqBoi7ooJhlwbau/AjKEgIqWWAlwkIwqLh2dcHv/nFfR55NniSHK3nOyeG8XzPPnPv+Ptd939edec755P51PakqJEnq8azZ7oAkae4yRCRJ3QwRSVI3Q0SS1M0QkSR122O2OzDT9ttvv1q8ePFsd0OS5pTbbrvt0apauGV93oXI4sWLWbt27Wx3Q5LmlCT/c1zd01mSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqdvEQiTJwUluSHJ3kvVJfrPVz0/y7SS3t9fxI8u8K8mGJN9I8saR+vJW25Dk3JH6S5LcnOSeJFck2WtS+yNJ2tokj0SeAH6nql4KHAWcneSw9t4Hq2ppe10L0N47BXgZsBz4aJIFSRYAHwGOAw4DTh1Zz3vbupYAjwFnTXB/JElbmFiIVNVDVfXVNv04cDdw0HYWOQG4vKp+VFX3ARuAI9prQ1XdW1U/Bi4HTkgS4A3AVW351cCJk9kbSdI4M/LEepLFwKuAm4GjgXOSnA6sZThaeYwhYG4aWWwjT4XOA1vUjwReBHy3qp4Y037L7a8EVgIccsghO7Uvh//by3ZqeT0z3fa+02e7CwB8692vmO0uaDd0yB+sm9i6J35hPcnzgU8B76yq7wMXA4cCS4GHgPdPNR2zeHXUty5WXVJVy6pq2cKFWw39IknqNNEjkSR7MgTIJ6rq0wBV9fDI+x8DPtdmNwIHjyy+CHiwTY+rPwrsnWSPdjQy2l6SNAMmeXdWgI8Dd1fVB0bqB440exNwZ5u+BjglybOTvARYAtwC3AosaXdi7cVw8f2aGr4c/gbgpLb8CuDqSe2PJGlrkzwSORp4K7Auye2t9vsMd1ctZTj1dD/wNoCqWp/kSuAuhju7zq6qJwGSnANcBywAVlXV+ra+3wMuT/JHwNcYQkuSNEMmFiJV9Q+Mv25x7XaWuRC4cEz92nHLVdW9DHdvSZJmgU+sS5K6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqNrEQSXJwkhuS3J1kfZLfbPV9k6xJck/7uU+rJ8mHkmxIckeSV4+sa0Vrf0+SFSP1w5Osa8t8KEkmtT+SpK1N8kjkCeB3quqlwFHA2UkOA84Frq+qJcD1bR7gOGBJe60ELoYhdIDzgCOBI4DzpoKntVk5stzyCe6PJGkLEwuRqnqoqr7aph8H7gYOAk4AVrdmq4ET2/QJwGU1uAnYO8mBwBuBNVW1uaoeA9YAy9t7L6yqr1RVAZeNrEuSNANm5JpIksXAq4CbgQOq6iEYggbYvzU7CHhgZLGNrba9+sYx9XHbX5lkbZK1mzZt2tndkSQ1Ew+RJM8HPgW8s6q+v72mY2rVUd+6WHVJVS2rqmULFy7cUZclSdM00RBJsidDgHyiqj7dyg+3U1G0n4+0+kbg4JHFFwEP7qC+aExdkjRDJnl3VoCPA3dX1QdG3roGmLrDagVw9Uj99HaX1lHA99rpruuAY5Ps0y6oHwtc1957PMlRbVunj6xLkjQD9pjguo8G3gqsS3J7q/0+8B7gyiRnAd8CTm7vXQscD2wAfgicCVBVm5NcANza2r27qja36bcDlwLPAT7fXpKkGTKxEKmqf2D8dQuAY8a0L+DsbaxrFbBqTH0t8PKd6KYkaSf4xLokqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG4TC5Ekq5I8kuTOkdr5Sb6d5Pb2On7kvXcl2ZDkG0neOFJf3mobkpw7Un9JkpuT3JPkiiR7TWpfJEnjTfJI5FJg+Zj6B6tqaXtdC5DkMOAU4GVtmY8mWZBkAfAR4DjgMODU1hbgvW1dS4DHgLMmuC+SpDEmFiJV9XfA5mk2PwG4vKp+VFX3ARuAI9prQ1XdW1U/Bi4HTkgS4A3AVW351cCJu3QHJEk7NBvXRM5Jckc73bVPqx0EPDDSZmOrbav+IuC7VfXEFnVJ0gya6RC5GDgUWAo8BLy/1TOmbXXUx0qyMsnaJGs3bdr09HosSdqmGQ2Rqnq4qp6sqp8AH2M4XQXDkcTBI00XAQ9up/4osHeSPbaob2u7l1TVsqpatnDhwl2zM5KkmQ2RJAeOzL4JmLpz6xrglCTPTvISYAlwC3ArsKTdibUXw8X3a6qqgBuAk9ryK4CrZ2IfJElP2WPHTfok+STwOmC/JBuB84DXJVnKcOrpfuBtAFW1PsmVwF3AE8DZVfVkW885wHXAAmBVVa1vm/g94PIkfwR8Dfj4pPZFkjTetEIkyfVVdcyOaqOq6tQx5W3+oa+qC4ELx9SvBa4dU7+Xp06HSZJmwXZDJMnPAM9lOJrYh6cuaL8QePGE+yZJ2s3t6EjkbcA7GQLjNp4Kke8zPAQoSZrHthsiVfVnwJ8leUdVfXiG+iRJmiOmdU2kqj6c5LXA4tFlquqyCfVLkjQHTPfC+n9ieEjwduDJVi7AEJGkeWy6t/guAw5rz2dIkgRM/2HDO4F/MsmOSJLmnukeiewH3JXkFuBHU8Wq+uWJ9EqSNCdMN0TOn2QnJElz03TvzvrSpDsiSZp7pnt31uM8NdT6XsCewP+uqhdOqmOSpN3fdI9EXjA6n+REHLdKkua9rqHgq+o/M3w9rSRpHpvu6aw3j8w+i+G5EZ8ZkaR5brp3Z/3SyPQTDN8FcsIu740kaU6Z7jWRMyfdEUnS3DOtayJJFiX5TJJHkjyc5FNJFk26c5Kk3dt0L6z/FcP3oL8YOAj4bKtJkuax6YbIwqr6q6p6or0uBRZOsF+SpDlguiHyaJLTkixor9OA70yyY5Kk3d90Q+TXgbcA/wt4CDgJ8GK7JM1z073F9wJgRVU9BpBkX+AihnCRJM1T0z0S+cWpAAGoqs3AqybTJUnSXDHdEHlWkn2mZtqRyHSPYiRJz1DTDYL3A/8tyVUMw528BbhwYr2SJM0J031i/bIkaxkGXQzw5qq6a6I9kyTt9qZ9SqqFhsEhSfqprqHgJUkCQ0SStBMMEUlSN0NEktTNEJEkdTNEJEndJhYiSVa1L7G6c6S2b5I1Se5pP/dp9ST5UJINSe5I8uqRZVa09vckWTFSPzzJurbMh5JkUvsiSRpvkkcilwLLt6idC1xfVUuA69s8wHHAkvZaCVwMPx1e5TzgSOAI4LyR4Vcubm2nlttyW5KkCZtYiFTV3wGbtyifAKxu06uBE0fql9XgJmDvJAcCbwTWVNXmNgDkGmB5e++FVfWVqirgspF1SZJmyExfEzmgqh4CaD/3b/WDgAdG2m1ste3VN46pj5VkZZK1SdZu2rRpp3dCkjTYXS6sj7ueUR31sarqkqpaVlXLFi70W30laVeZ6RB5uJ2Kov18pNU3AgePtFsEPLiD+qIxdUnSDJrpELkGmLrDagVw9Uj99HaX1lHA99rpruuAY5Ps0y6oHwtc1957PMlR7a6s00fWJUmaIRP7YqkknwReB+yXZCPDXVbvAa5MchbwLeDk1vxa4HhgA/BD2ve3V9XmJBcAt7Z2727fqgjwdoY7wJ4DfL69JEkzaGIhUlWnbuOtY8a0LeDsbaxnFbBqTH0t8PKd6aMkaefsLhfWJUlzkCEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkbrMSIknuT7Iuye1J1rbavknWJLmn/dyn1ZPkQ0k2JLkjyatH1rOitb8nyYrZ2BdJms9m80jk9VW1tKqWtflzgeuraglwfZsHOA5Y0l4rgYthCB3gPOBI4AjgvKngkSTNjN3pdNYJwOo2vRo4caR+WQ1uAvZOciDwRmBNVW2uqseANcDyme60JM1nsxUiBXwxyW1JVrbaAVX1EED7uX+rHwQ8MLLsxlbbVn0rSVYmWZtk7aZNm3bhbkjS/LbHLG336Kp6MMn+wJokX99O24yp1XbqWxerLgEuAVi2bNnYNpKkp29WjkSq6sH28xHgMwzXNB5up6loPx9pzTcCB48svgh4cDt1SdIMmfEQSfK8JC+YmgaOBe4ErgGm7rBaAVzdpq8BTm93aR0FfK+d7roOODbJPu2C+rGtJkmaIbNxOusA4DNJprb/N1X1hSS3AlcmOQv4FnBya38tcDywAfghcCZAVW1OcgFwa2v37qraPHO7IUma8RCpqnuBV46pfwc4Zky9gLO3sa5VwKpd3UdJ0vTsTrf4SpLmGENEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVK3OR8iSZYn+UaSDUnOne3+SNJ8MqdDJMkC4CPAccBhwKlJDpvdXknS/DGnQwQ4AthQVfdW1Y+By4ETZrlPkjRv7DHbHdhJBwEPjMxvBI7cslGSlcDKNvuDJN+Ygb7NB/sBj852J3YHuWjFbHdBW/PzOeW87Iq1/Ny44lwPkXH/MrVVoeoS4JLJd2d+SbK2qpbNdj+kcfx8zoy5fjprI3DwyPwi4MFZ6oskzTtzPURuBZYkeUmSvYBTgGtmuU+SNG/M6dNZVfVEknOA64AFwKqqWj/L3ZpPPEWo3ZmfzxmQqq0uIUiSNC1z/XSWJGkWGSKSpG6GiLok+Y0kp7fpM5K8eOS9v3TkAO1Okuyd5N+MzL84yVWz2adnCq+JaKcluRH43apaO9t9kcZJshj4XFW9fJa78ozjkcg8lGRxkq8nWZ3kjiRXJXlukmOSfC3JuiSrkjy7tX9Pkrta24ta7fwkv5vkJGAZ8Ikktyd5TpIbkyxL8vYkfzqy3TOSfLhNn5bklrbMX7Rx0DRPtc/k3Uk+lmR9ki+2z9KhSb6Q5LYkf5/kF1r7Q5PclOTWJO9O8oNWf36S65N8tX2Op4ZBeg9waPu8va9t7862zM1JXjbSlxuTHJ7kee334Nb2e+GQSuNUla959gIWMzzZf3SbXwX8B4YhZH6+1S4D3gnsC3yDp45a924/z2c4+gC4EVg2sv4bGYJlIcPYZlP1zwP/Angp8Flgz1b/KHD6bP+7+Jr1z+QTwNI2fyVwGnA9sKTVjgT+a5v+HHBqm/4N4Adteg/ghW16P2ADw8gWi4E7t9jenW36t4A/bNMHAt9s038MnNam9wa+CTxvtv+tdreXRyLz1wNV9eU2/dfAMcB9VfXNVlsN/Cvg+8D/Af4yyZuBH053A1W1Cbg3yVFJXgT8c+DLbVuHA7cmub3N/9NdsE+a2+6rqtvb9G0Mf+hfC/xt+5z8BcMfeYDXAH/bpv9mZB0B/jjJHcB/YRhf74AdbPdK4OQ2/ZaR9R4LnNu2fSPwM8AhT3uvnuHm9MOG2inTuhhWwwOdRzD8oT8FOAd4w9PYzhUMv5hfBz5TVZUkwOqqetfT7LOe2X40Mv0kwx//71bV0qexjl9jOAI+vKr+b5L7Gf74b1NVfTvJd5L8IvCrwNvaWwF+paocsHU7PBKZvw5J8po2fSrD/9oWJ/lnrfZW4EtJng/8bFVdy3B6a9wv9OPAC7axnU8DJ7ZtXNFq1wMnJdkfIMm+ScaOEKp57fvAfUlOBsjgle29m4BfadOnjCzzs8AjLUBez1Mjz27vMwrD10j8O4bP+rpWuw54R/tPD0letbM79ExkiMxfdwMr2mH/vsAHgTMZTh2sA34C/DnDL97nWrsvMZw/3tKlwJ9PXVgffaOqHgPuAn6uqm5ptbsYrsF8sa13DU+dppBG/RpwVpL/Dqznqe8Leifw20luYfjsfK/VPwEsS7K2Lft1gKr6DvDlJHcmed+Y7VzFEEZXjtQuAPYE7mgX4S/YpXv2DOEtvvOQtztqrkvyXOAf2+nRUxgusnv31Czwmoikuehw4D+2U03fBX59lvszb3kkIknq5jURSVI3Q0SS1M0QkSR1M0SkGZJkaZLjR+Z/Ocm5E97m65K8dpLb0PxmiEgzZynw0xCpqmuq6j0T3ubrGIYOkSbCu7OkaUjyPIYH0RYBCxgePNsAfAB4PvAocEZVPdSGxr8ZeD3DwH1ntfkNwHOAbwN/0qaXVdU5SS4F/hH4BYanrM8EVjCMEXVzVZ3R+nEs8IfAs4H/AZxZVT9ow3usBn6J4QG5kxnGPLuJYQiRTcA7qurvJ/Hvo/nLIxFpepYDD1bVK9tDml8APgycVFWHM4yEfOFI+z2q6giGJ6vPq6ofA38AXFFVS6vqCra2D8O4ZL/FMMrxB4GXAa9op8L2Y3jS/19X1auBtcBvjyz/aKtfzDDC8v0Mow58sG3TANEu58OG0vSsAy5K8l6GYcgfA14OrGlDKy0AHhpp/+n2c2o02un4bHsCex3w8NQYTknWt3UsAg5jGL4DYC/gK9vY5pufxr5J3QwRaRqq6ptJDme4pvEnDON9ra+q12xjkakRaZ9k+r9nU8v8hP9/RNuftHU8CaypqlN34TalneLpLGkaMnyH/A+r6q+Bixi+IGnh1EjISfYc/Xa8bdjRSLI7chNw9NRIyxm+jfLnJ7xNabsMEWl6XgHc0r6g6N8zXN84CXhvG2H2dnZ8F9QNwGFttONffbodaF/ydQbwyTb68U0MF+K357PAm9o2/+XT3aa0I96dJUnq5pGIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuv0/WdK2PDux2LkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=\"sentiment\",data=review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now preprocessing our data so that we can remove the html tags,punctuations etc.\n",
    "def preprocessing_data(sentence):\n",
    "    #Removing the html tags\n",
    "    reviews=removing_tags(sentence)\n",
    "    #Removing every character except the alphabets from a to z(both in lower and upper case)\n",
    "    reviews=re.sub(\"[^a-zA-Z]\",\" \",reviews)\n",
    "    #As we are removing the apostrophes what actually will happen is that word like sourabh's will get replaces by sourabh and s as separate words\n",
    "    #Our regular will be creating single words(one character) which are of no importance so we remove them\n",
    "    #Removing the single character as they are not important\n",
    "    reviews=re.sub(\"\\s+[a-zA-Z]\\s+\",\" \",reviews)\n",
    "    #Removing multiple spaces from reviews\n",
    "    reviews=re.sub(\"\\s+\",\" \",reviews)\n",
    "    #Making the text lowercase\n",
    "    reviews=reviews.lower()\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAR_RE = re.compile(r\"<[^>]+>\")\n",
    "#Function for removing tags\n",
    "def removing_tags(text):\n",
    "    return TAR_RE.sub(\"\",text)\n",
    "#The removing tags function simply replaces anything between the < and > with empty space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "#Now creating the corpus of text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "corpus = []\n",
    "sentences = list(review_data[\"review\"])\n",
    "for sent in sentences:\n",
    "    corpus.append(preprocessing_data(sent))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the sentiment column to ones and zeros\n",
    "y = review_data[\"sentiment\"]\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(corpus,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our embedding layer which will convert our preprocessed textual data into numeric data\n",
    ".As a first step, we will use the Tokenizer class from the keras.preprocessing.text module to create a word-to-index dictionary. In the word-to-index dictionary, each word in the corpus is used as a key, while a corresponding unique index is used as the value for the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using TF-IDF and count vectorizer as they neither capture positions in text and nor they capture the meaning in text and we will be using Glove pretrained as it capture position in text as well as meaning and also it is trained on a lagre corpus or we can also use fast text for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "#Here the x_train will contain the index of word from the tokenizer\n",
    "#After this step x_train and x_test elements will have different lengths depending upon the element size(sentence size)\n",
    "#So we need to perform padding so as to maintain fixed length of the sequences inx_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing padding on both train and test set and setting size to be 100 of sequence\n",
    "#And if a sequence has more than maxlen the sequence will be truncated after maxlen\n",
    "#And if sequence has lesser len than maxlen the sequence will be padded with zeros to make sequence length equal to maxlen\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n",
    "x_train = pad_sequences(x_train, padding='post',maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post',maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "#We will use GloVe embeddings to create our feature matrix and we will create a dictionary that will contain words as keys and their corresponding embedding list as values\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open(\"glove.6B.100d.txt\", encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.073695 -0.033995  0.1175   -0.62037   0.02282  -0.54719   0.17815\n",
      " -0.65397  -0.31618  -1.1676   -0.56243   0.65186   0.030927  0.007971\n",
      "  0.32896   0.56995   0.52986  -0.094319 -0.61012   0.23755  -0.14946\n",
      "  0.11913   0.059727  0.2162    0.36176   0.93062  -0.92673   0.36767\n",
      "  0.059147  0.3082    0.17613   0.026632 -0.45234   0.27703   0.18998\n",
      "  0.26091  -0.026159  0.40505   1.3821    0.075603 -0.076359  0.19258\n",
      " -0.56764   0.71397   0.10174  -0.086357 -0.15671  -0.53022  -0.1063\n",
      "  0.32784  -0.066472 -0.5361   -0.13433   0.61026  -0.21795  -1.4938\n",
      "  0.14192   0.12176   0.38179   0.35812   0.76489   0.69533  -0.11341\n",
      " -0.055595  1.0308   -0.084502  0.20062   0.57746   0.15714   0.57664\n",
      " -0.26053  -0.39672  -0.24518   0.042514 -0.1222    0.082824 -0.56566\n",
      " -0.64397  -0.28528   0.44216  -0.17003  -0.16883  -0.34474  -0.20922\n",
      " -0.38548   0.072244 -0.096744 -0.53621   0.54864   0.50515   0.45365\n",
      " -0.82197   0.51722   0.36603   0.040918 -0.20183  -0.92315   0.47886\n",
      "  0.6582   -0.27819 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"wolf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create an embedding matrix where each row number will correspond to the index of the word in the corpus. The matrix will have 100 columns where each column will contain the GloVe word embeddings for the words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Now performing the classification task\n",
    "#Here first we are going to take a simple neural network(yhen proceed to advance)\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "#For a deep neural network the input layer can be TF-IDF, word embeddings etc\n",
    "#We are not training our own embeddings and using the GloVe embedding\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          9254700   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 14,506,797\n",
      "Trainable params: 5,252,097\n",
      "Non-trainable params: 9,254,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 29s 892us/step - loss: 0.7013 - acc: 0.6144 - val_loss: 0.5701 - val_acc: 0.7181\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 26s 804us/step - loss: 0.5680 - acc: 0.7004 - val_loss: 0.5439 - val_acc: 0.7284\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 27s 849us/step - loss: 0.5305 - acc: 0.7224 - val_loss: 0.5225 - val_acc: 0.7365\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 28s 881us/step - loss: 0.4876 - acc: 0.7542 - val_loss: 0.5301 - val_acc: 0.7319\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 27s 845us/step - loss: 0.4747 - acc: 0.7596 - val_loss: 0.5437 - val_acc: 0.7322\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 30s 930us/step - loss: 0.4618 - acc: 0.7605 - val_loss: 0.5261 - val_acc: 0.7371\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "#As our accuracy is still increasing and loss is coming down, we can icrease the number of layers in our neural network and see how it is helping in getting model accurate\n",
    "#But better will be to use RNN's,CNN's,RCNN,Attention network and other architectures that work good for such problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 170us/step\n",
      "Test Accuracy: 0.7396\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "#As our accuracy so we need to use some other architecture to improve accuracy and get a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks(for hierarchical document classification)have been found to work well with text data as well.Though text data is one-dimensional,we can use 1D convolutional neural networks to extract features from our data\n",
    "\n",
    "Although originally was built for image processing with architecture similar to that of the the visual cortex,CNNs have also been effectively used for text classification and related task\n",
    "\n",
    "Problem of CNN using with text is the number of channels,Sigma(size of the feature space).This might be very large (nK-n a large number take forexample 80),for text but for images this is less of a problem (only 3 channels of RGB).This basically means the dimensionality of the CNN for text is very high.\n",
    "\n",
    "Here we will be using convolutonal 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bluebrain/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          9254700   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 96, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              394240    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 10,320,429\n",
      "Trainable params: 1,065,729\n",
      "Non-trainable params: 9,254,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "model1 = Sequential()\n",
    "embedding_layer1 = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model1.add(embedding_layer1)\n",
    "model1.add(Conv1D(128, 5, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(MaxPooling1D(5))\n",
    "model1.add(Conv1D(128, 5, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(MaxPooling1D(5))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1024, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 29s 909us/step - loss: 0.6120 - acc: 0.6583 - val_loss: 0.5060 - val_acc: 0.7804\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 20s 622us/step - loss: 0.4584 - acc: 0.7853 - val_loss: 0.4593 - val_acc: 0.8085\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 20s 627us/step - loss: 0.4197 - acc: 0.8079 - val_loss: 0.4511 - val_acc: 0.8204\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 21s 669us/step - loss: 0.3968 - acc: 0.8210 - val_loss: 0.4240 - val_acc: 0.8271\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 22s 700us/step - loss: 0.3666 - acc: 0.8364 - val_loss: 0.3994 - val_acc: 0.8197\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 22s 681us/step - loss: 0.3511 - acc: 0.8474 - val_loss: 0.4066 - val_acc: 0.8323\n",
      "10000/10000 [==============================] - 2s 192us/step\n",
      "Test Accuracy: 0.8313\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(x_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model1.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 100)          9254700   \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 100, 256)          274176    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 100, 128)          147840    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,763,437\n",
      "Trainable params: 508,737\n",
      "Non-trainable params: 9,254,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Using recurrent neural network for classification as basically neural network works well with sequential data\n",
    "#As text is also a sequential data so the best choice is to use recurrent neural network\n",
    "from keras.layers import LSTM,GRU\n",
    "model2 = Sequential()\n",
    "embedding_layer2 = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model2.add(embedding_layer2)\n",
    "model2.add(GRU(256,return_sequences = True,recurrent_dropout = 0.2))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(GRU(128,return_sequences = True,recurrent_dropout = 0.2))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(GRU(64,recurrent_dropout = 0.2))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(256, activation=\"relu\"))\n",
    "model2.add(Dense(128, activation=\"relu\"))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))\n",
    "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 177s 6ms/step - loss: 0.5435 - acc: 0.7182 - val_loss: 0.4196 - val_acc: 0.8055\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 157s 5ms/step - loss: 0.4119 - acc: 0.8165 - val_loss: 0.3828 - val_acc: 0.8376\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 154s 5ms/step - loss: 0.3732 - acc: 0.8359 - val_loss: 0.3627 - val_acc: 0.8425\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 160s 5ms/step - loss: 0.3579 - acc: 0.8458 - val_loss: 0.3486 - val_acc: 0.8481\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 155s 5ms/step - loss: 0.3218 - acc: 0.8616 - val_loss: 0.3296 - val_acc: 0.8585\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 158s 5ms/step - loss: 0.3094 - acc: 0.8698 - val_loss: 0.3267 - val_acc: 0.8601\n",
      "10000/10000 [==============================] - 20s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(x_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model2.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8631\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RCNN for the task of classification\n",
    "\n",
    "The main idea of this technique is capturing contextual information with the recurrent structure and constructing the representation of text using a convolutional neural network.This architecture is a combination of RNN and CNN to use advantages of both technique in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 100, 100)          9254700   \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 96, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 3, 256)            394240    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 3, 128)            197120    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 3, 64)             49408     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 10,777,005\n",
      "Trainable params: 1,522,305\n",
      "Non-trainable params: 9,254,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "embedding_layer3 = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model3.add(embedding_layer3)\n",
    "model3.add(Conv1D(128, 5, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(MaxPooling1D(5))\n",
    "model3.add(Conv1D(128, 5, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(MaxPooling1D(5))\n",
    "model3.add(LSTM(256, return_sequences=True, recurrent_dropout=0.2))\n",
    "model3.add(LSTM(128, return_sequences=True, recurrent_dropout=0.2))\n",
    "model3.add(LSTM(64, return_sequences=True, recurrent_dropout=0.2))\n",
    "model3.add(LSTM(32, recurrent_dropout=0.2))\n",
    "model3.add(Dense(1024,activation=\"relu\"))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(512,activation=\"relu\"))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(256,activation=\"relu\"))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(128,activation=\"relu\"))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(1,activation=\"sigmoid\"))   \n",
    "model3.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "32000/32000 [==============================] - 39s 1ms/step - loss: 0.6905 - acc: 0.5135 - val_loss: 0.6537 - val_acc: 0.6499\n",
      "Epoch 2/6\n",
      "32000/32000 [==============================] - 29s 901us/step - loss: 0.5210 - acc: 0.7521 - val_loss: 0.4508 - val_acc: 0.8004\n",
      "Epoch 3/6\n",
      "32000/32000 [==============================] - 27s 857us/step - loss: 0.4354 - acc: 0.8036 - val_loss: 0.4767 - val_acc: 0.7751\n",
      "Epoch 4/6\n",
      "32000/32000 [==============================] - 27s 842us/step - loss: 0.3987 - acc: 0.8211 - val_loss: 0.3832 - val_acc: 0.8280\n",
      "Epoch 5/6\n",
      "32000/32000 [==============================] - 27s 847us/step - loss: 0.3815 - acc: 0.8321 - val_loss: 0.3695 - val_acc: 0.8346\n",
      "Epoch 6/6\n",
      "32000/32000 [==============================] - 27s 844us/step - loss: 0.3529 - acc: 0.8468 - val_loss: 0.3703 - val_acc: 0.8413\n",
      "10000/10000 [==============================] - 4s 369us/step\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(x_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model3.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8381\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above models we got best accuracy with Recurrent Neural Network\n",
    "\n",
    "All the accuray stuff depends upon the architecture, the number of layers used, the number of neuron used in each layer, the hyperparameters etc\n",
    "\n",
    "We can also use RMDL(Random Multimodel deep learning),HDLTex(Hierarchical deep learning for text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
